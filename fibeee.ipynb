{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9480627,"sourceType":"datasetVersion","datasetId":5766698}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/fffiiibbbeee/train.csv\" ,  encoding = 'latin')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the dataset\ndf = pd.read_csv(\"/kaggle/input/fffiiibbbeee/train.csv\", encoding='latin')\n\n# Check the distribution of the target variable to confirm counts\ncategory_counts = df['target'].value_counts()\n\n# Find the second highest number of samples\nsecond_highest_count = category_counts.sort_values(ascending=False).iloc[1]\n\n# Downsample the 'academic interests' category\ndf_academic = df[df['target'] == 'academic interests']\ndf_academic_downsampled = df_academic.sample(n=second_highest_count, random_state=42)\n\n# Filter out the old 'academic interests' entries from the original dataframe\ndf_rest = df[df['target'] != 'academic interests']\n\n# Concatenate the downsampled 'academic interests' dataframe with the rest of the data\ndf_final = pd.concat([df_rest, df_academic_downsampled])\n\n# Optionally, shuffle the dataset if necessary\ndf_final = df_final.sample(frac=1).reset_index(drop=True)\n\ndf = df_final\n\n# Print the new count of 'academic interests' to confirm\nplt.figure(figsize=(8,6))\nsns.countplot(y='target', data=df)\nplt.title('Target Variable Distribution')\nplt.ylabel('Target')\nplt.xlabel('Count')\nplt.show()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split \n\nX_train, X_val, y_train, y_val = train_test_split(df[\"text\"],df[\"target\"] , \n                                   random_state=104,  \n                                   test_size=0.25,  \n                                   shuffle=True) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\none_hot_encoder = OneHotEncoder(sparse_output = False)\ntrain_labels_one_hot  = one_hot_encoder.fit_transform(y_train.to_numpy().reshape(-1,1))\nval_labels_one_hot = one_hot_encoder.transform(y_val.to_numpy().reshape(-1,1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ntrain_labels_encoded  = label_encoder.fit_transform(y_train.to_numpy())\nval_labels_encoded = label_encoder.transform(y_val.to_numpy())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_classes = len(label_encoder.classes_)\nclass_names = label_encoder.classes_\n\nnum_classes , class_names","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sent_lens = [len(sen.split()) for sen in X_train]\noutput_seq_len = int(np.percentile(sent_lens , 95))\nmax_tokens = 68000","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import TextVectorization\n\ntext_vectorizer = TextVectorization(\n    max_tokens = max_tokens,\n    output_sequence_length = output_seq_len\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_vectorizer.adapt(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab = text_vectorizer.get_vocabulary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token_embed = layers.Embedding(input_dim = len(vocab),\n                               output_dim = 128,\n                               mask_zero = True,\n                               name = \"token_embedding\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = tf.data.Dataset.from_tensor_slices((X_train,train_labels_one_hot ))\nvalid_dataset = tf.data.Dataset.from_tensor_slices((X_val,val_labels_one_hot ))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset  = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\nvalid_dataset = valid_dataset.batch(32).prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = layers.Input(shape = (1,), dtype = tf.string)\ntext_vectors = text_vectorizer(inputs)\ntoken_embeddings = token_embed(text_vectors)\nx = layers.Conv1D(128, 5, activation='relu')(token_embeddings)\nx = layers.MaxPooling1D(pool_size=2)(x)\nx = layers.Conv1D(128, 5, activation='relu')(x)\nx = layers.GlobalMaxPooling1D()(x)\nx = layers.Dense(128, activation='relu')(x)\noutputs = layers.Dense(26, activation='softmax')(x)\n\nmodel_1 = tf.keras.Model(inputs , outputs)\n\nmodel_1.compile(\n    loss = \"categorical_crossentropy\",\n    optimizer = \"Adam\",\n    metrics =[\"accuracy\"]\n)\n\nmodel_1.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_model_1 = model_1.fit(\n                     train_dataset,\n                     steps_per_epoch = int(0.1*len(train_dataset)),\n                     epochs = 10,\n                     validation_data = valid_dataset,\n                     validation_steps = int(0.1*len(valid_dataset))\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1_pred_probs = model_1.predict(valid_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1_preds = tf.argmax(model_1_pred_probs , axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score\nprint(f1_score(val_labels_encoded, model_1_preds , average='weighted'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv(\"/kaggle/input/fffiiibbbeee/test.csv\" ,  encoding = 'latin')\ndf_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\n\n\n# Extract the 'text' column for prediction\ntest_texts = test_data['text'].tolist()\n\n# Create a TensorFlow Dataset\ntest_dataset = tf.data.Dataset.from_tensor_slices(test_texts)\ntest_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n\n# Make predictions\npredictions = model_1.predict(test_dataset)\n\n# Get the predicted class indices\npredicted_class_indices = np.argmax(predictions, axis=1)\n\n# Convert class indices back to original labels\npredicted_labels = label_encoder.inverse_transform(predicted_class_indices)\n\n# Add the predicted labels to the test DataFrame\ntest_data['target'] = predicted_labels\n\n# Save the DataFrame to a new CSV file\noutput_file = 'predictions.csv'\ntest_data.to_csv(output_file, index=False)\n\nprint(f\"Predictions saved to {output_file}\")\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"op = pd.read_csv(\"/kaggle/working/predictions.csv\" ,  encoding = 'latin')\nop.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"op = op.drop(['text', 'Word Count'], axis=1)\nop.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"op = op[[ 'target','Index']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"op.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"op.to_csv(\"final_test.csv\", index=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}